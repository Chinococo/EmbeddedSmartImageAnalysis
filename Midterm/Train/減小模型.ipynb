{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 確認模型精度",
   "id": "c25d1a0b905dd9af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# 指定 .pth 檔案路徑\n",
    "model_path = \"../Result/resnet18_1600_v4.pth\"\n",
    "\n",
    "# 建立 ResNet-18 模型結構並載入權重\n",
    "model = models.resnet18()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))  # 只載入權重\n",
    "\n",
    "# 定義函數來檢查模型的資料型別\n",
    "def check_dtype(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        dtype = param.dtype\n",
    "        print(f\"Layer: {name}, Data Type: {dtype}\")\n",
    "\n",
    "# 呼叫函數檢查模型的資料型別\n",
    "check_dtype(model)\n"
   ],
   "id": "99e45913059e0c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加載fintuning資料集\n",
   "id": "1dced156fbac047e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T00:06:31.089775Z",
     "start_time": "2024-11-11T00:06:30.862088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width / 2) / (width / 2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height / 2) / (height / 2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "        \n",
    "        # 裁切掉上半部 40%，只保留下半部的 60%\n",
    "        cropped_image = image.crop((0, int(height * 0.4), width, height))  # 保留從 40% 開始到底部的部分\n",
    "        \n",
    "        # 水平翻轉圖片（若隨機觸發）\n",
    "        if float(np.random.rand(1)) > 0.5 and self.random_hflips:\n",
    "            cropped_image = transforms.functional.hflip(cropped_image)\n",
    "            x = -x\n",
    "        \n",
    "        # 應用顏色抖動\n",
    "        cropped_image = self.color_jitter(cropped_image)\n",
    "        \n",
    "        # 調整圖片大小至 224x134\n",
    "        cropped_image = transforms.functional.resize(cropped_image, (134, 224))\n",
    "        \n",
    "        # 轉換為 tensor 並進行標準化\n",
    "        image_tensor = transforms.functional.to_tensor(cropped_image)\n",
    "        image_tensor = transforms.functional.normalize(image_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image_tensor, torch.tensor([x, y]).float()\n",
    "    \n",
    "# 建立資料集實例\n",
    "dataset = XYDataset('1600-v4', random_hflips=False)\n",
    "print(dataset[0][1])\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "4860ee74d006bfca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9464,  0.7143])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\581405686.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if float(np.random.rand(1)) > 0.5 and self.random_hflips:\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 重新調整精度(fp32->fp16) 剪枝10%",
   "id": "966533fcccdb758f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T00:59:37.589732Z",
     "start_time": "2024-11-11T00:52:47.129263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.utils import prune\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 指定 .pth 檔案路徑\n",
    "model_path = \"../Result/resnet18_1600_v4.pth\"\n",
    "\n",
    "# 建立 ResNet-18 模型結構並載入權重\n",
    "model = models.resnet18()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)  # 忽略 fc 層的形狀不匹配\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)  # 修改最後一層輸出為 2，以匹配 [x, y]\n",
    "\n",
    "\n",
    "# 對模型的卷積層進行剪枝\n",
    "def apply_pruning(module, amount=0.1):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "\n",
    "\n",
    "model.apply(lambda module: apply_pruning(module, amount=0.1))\n",
    "\n",
    "# 檢查是否有 GPU，並將模型移動到 GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# 使用自動混合精度和 GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 將模型設置為訓練模式\n",
    "model.train()\n",
    "\n",
    "# 定義優化器和 MSE 損失函數\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)  # 調整學習率\n",
    "criterion = MSELoss()  # 使用 MSELoss\n",
    "\n",
    "# 微調模型\n",
    "for epoch in range(10):  # 訓練 10 個 epoch，可以根據需要調整\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 優化器梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 使用 autocast 進行自動混合精度訓練\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # 進行反向傳播\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/10], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 移除剪枝掩碼，將剪枝後的權重變為永久權重\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "# 儲存微調後的模型\n",
    "torch.save(model.state_dict(), \"resnet18_finetuned_fp16_pruned.pth\")\n"
   ],
   "id": "c6823c4e893dc9fd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\170469883.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)  # 忽略 fc 層的形狀不匹配\n",
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\170469883.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\581405686.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if float(np.random.rand(1)) > 0.5 and self.random_hflips:\n",
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\170469883.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0548\n",
      "Epoch [2/10], Loss: 0.0269\n",
      "Epoch [3/10], Loss: 0.0216\n",
      "Epoch [4/10], Loss: 0.0157\n",
      "Epoch [5/10], Loss: 0.0125\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [7/10], Loss: 0.0079\n",
      "Epoch [8/10], Loss: 0.0069\n",
      "Epoch [9/10], Loss: 0.0065\n",
      "Epoch [10/10], Loss: 0.0061\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 測試結果",
   "id": "9ed69f597469473a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T01:02:02.759390Z",
     "start_time": "2024-11-11T01:00:00.596098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "\n",
    "# 設定資料夾路徑\n",
    "image_folder = \"1600-v4\"\n",
    "image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 設定 widget 以顯示圖片\n",
    "widget_width = 224\n",
    "widget_height = 224\n",
    "image_widget = widgets.Image(format='jpeg', width=widget_width, height=widget_height)\n",
    "display(image_widget)\n",
    "\n",
    "# 設定裝置 (若有 GPU 可用)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 加載模型\n",
    "model = models.resnet18()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(\"resnet18_finetuned_fp16_pruned.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()  # 將模型設定為推論模式\n",
    "\n",
    "# 圖片預處理和顯示函數\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4691, 0.4032, 0.4579], [0.1740, 0.1485, 0.1688])\n",
    "])\n",
    "\n",
    "def bgr8_to_jpeg(image):\n",
    "    _, jpeg = cv2.imencode('.jpg', image)\n",
    "    return jpeg.tobytes()\n",
    "\n",
    "def process_image(image_path):\n",
    "    # 讀取圖片並進行裁切\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # 裁切圖片的下部 60%\n",
    "    cropped_image = image[int(height * 0.4):, :, :]  # 保留從 40% 開始到高度底部的部分\n",
    "\n",
    "    # 調整裁切後的圖片大小為 224x134\n",
    "    cropped_image = cv2.resize(cropped_image, (224, 134))\n",
    "    \n",
    "    # 預處理圖片\n",
    "    input_tensor = transform(cropped_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        x, y = output[0].cpu().numpy()\n",
    "    print(x,y/2)\n",
    "    # 映射預測的 (x, y) 到圖片的像素坐標，假設 x 和 y 是 [0, 1] 範圍內的預測\n",
    "    x_pixel = int(x * 224 / 2 + 224 / 2)\n",
    "    y_pixel = int(y * 134 / 2 + 134 / 2)\n",
    "    \n",
    "    # 在圖片上繪製預測結果\n",
    "    display_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)  # 確保格式正確\n",
    "    cv2.circle(display_image, (x_pixel, y_pixel), 5, (0, 255, 0), -1)  # 綠色點表示預測位置\n",
    "    \n",
    "    # 更新 widget 顯示處理後的圖片\n",
    "    image_widget.value = bgr8_to_jpeg(display_image)\n",
    "\n",
    "# 對資料夾中的每張圖片進行處理\n",
    "for image_file in image_files:\n",
    "    process_image(image_file)\n",
    "    time.sleep(2)  # 暫停以觀察每張圖片的結果\n"
   ],
   "id": "186b3f74bf85d353",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24765fa3cad5425abce98f19addb1a6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_91508\\705658317.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"resnet18_finetuned_fp16_pruned.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9946802 0.36236900091171265\n",
      "-0.74837273 0.4469776749610901\n",
      "-0.76901275 0.2802664637565613\n",
      "-0.9104858 0.3827850818634033\n",
      "-0.85545534 0.3716302514076233\n",
      "-0.76147217 0.3913075923919678\n",
      "-0.95470256 0.3939242362976074\n",
      "-0.9038053 0.3488048315048218\n",
      "-0.93339026 0.3653039038181305\n",
      "-0.7635399 0.4303571283817291\n",
      "-0.9383551 0.332721084356308\n",
      "-0.90690684 0.3390689492225647\n",
      "-0.73828983 0.3206668794155121\n",
      "-0.89149547 0.35504066944122314\n",
      "-0.9136929 0.36700886487960815\n",
      "-0.67804617 0.37622275948524475\n",
      "-1.0230854 0.42201387882232666\n",
      "-0.75872666 0.33321619033813477\n",
      "-0.87140626 0.3860875368118286\n",
      "-0.7770744 0.41226324439048767\n",
      "-0.91841805 0.38596639037132263\n",
      "-0.7491264 0.3910526633262634\n",
      "-0.7458859 0.43500620126724243\n",
      "-0.90625244 0.34816521406173706\n",
      "-0.8016814 0.40036216378211975\n",
      "-0.7434091 0.42813828587532043\n",
      "-0.6336264 0.3365474045276642\n",
      "-0.84745353 0.36397385597229004\n",
      "-0.64746064 0.3600819706916809\n",
      "-0.8224631 0.3465706408023834\n",
      "-0.7646155 0.32444465160369873\n",
      "-0.7848154 0.39144590497016907\n",
      "-0.72110635 0.40269720554351807\n",
      "-0.5295742 0.37136584520339966\n",
      "-1.001845 0.32297372817993164\n",
      "-0.73376036 0.3077167868614197\n",
      "-0.7547407 0.34806495904922485\n",
      "-0.8812139 0.36313098669052124\n",
      "-0.67093915 0.3845014274120331\n",
      "-0.73288554 0.36698636412620544\n",
      "-0.76929843 0.4164760410785675\n",
      "-0.8479883 0.4480210244655609\n",
      "-0.923451 0.38799309730529785\n",
      "-0.67620337 0.34912556409835815\n",
      "-0.73890346 0.399340957403183\n",
      "-0.73083586 0.4044781029224396\n",
      "-0.69516826 0.33366861939430237\n",
      "-0.8027969 0.27519482374191284\n",
      "-0.789412 0.3451334238052368\n",
      "-0.58041364 0.36583131551742554\n",
      "-0.6107138 0.3347044885158539\n",
      "-0.64342415 0.3758331835269928\n",
      "-0.7921664 0.4082412123680115\n",
      "-0.7294176 0.36191099882125854\n",
      "-0.68360084 0.4036248028278351\n",
      "-0.7275488 0.31431758403778076\n",
      "-0.614039 0.3715585470199585\n",
      "-0.8430318 0.4066970646381378\n",
      "-0.8544412 0.40470635890960693\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 75\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image_file \u001B[38;5;129;01min\u001B[39;00m image_files:\n\u001B[0;32m     74\u001B[0m     process_image(image_file)\n\u001B[1;32m---> 75\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 暫停以觀察每張圖片的結果\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28c5d97103b185d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
