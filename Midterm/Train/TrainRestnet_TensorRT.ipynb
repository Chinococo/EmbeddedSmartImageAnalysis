{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 解壓縮"
   ],
   "metadata": {
    "id": "uhXnDkISVlwt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf dataset_xy\n",
    "!rm -rf output\n",
    "!unzip -q road_following_dataset_xy_2024-11-03_12-08-48.zip"
   ],
   "metadata": {
    "id": "lZvXpWVlF7YM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 函式庫"
   ],
   "metadata": {
    "id": "CO7CCWuwVp88"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip uninstall scipy\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T16:26:58.029104Z",
     "start_time": "2024-11-05T16:26:52.541796Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install scipy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.sjtu.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: scipy in c:\\users\\chino\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\chino\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scipy) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Chino\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T07:17:06.615915Z",
     "start_time": "2024-11-07T07:17:03.178727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip3 --version\n",
    "!pip3 install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.3.1 from C:\\Users\\Chino\\Documents\\Methylation_analysis_prostate_cancer\\venv\\lib\\site-packages\\pip (python 3.10)\n",
      "\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.5.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: torchvision==0.20.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (0.20.0+cu121)\n",
      "Requirement already satisfied: torchaudio==2.5.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: networkx in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (2024.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torch==2.5.0) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torchvision==0.20.0) (10.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from torchvision==0.20.0) (1.26.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chino\\documents\\methylation_analysis_prostate_cancer\\venv\\lib\\site-packages (from jinja2->torch==2.5.0) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Chino\\Documents\\Methylation_analysis_prostate_cancer\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WytwDlFLzzao",
    "ExecuteTime": {
     "end_time": "2024-11-10T01:12:26.616889Z",
     "start_time": "2024-11-10T01:11:52.281392Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 檔案格式解碼\n"
   ],
   "metadata": {
    "id": "l3HVnfr7V0rZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('1100', random_hflips=False)\n",
    "print(dataset[0][1])"
   ],
   "metadata": {
    "id": "Gnx-I27T0p03",
    "ExecuteTime": {
     "end_time": "2024-11-10T01:12:27.059507Z",
     "start_time": "2024-11-10T01:12:26.643928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9018, 0.7054])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_61936\\3035275046.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if float(np.random.rand(1)) > 0.5:\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 區分訓練集\n"
   ],
   "metadata": {
    "id": "uo-5PILaDyPR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_percent = 0.2\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ],
   "metadata": {
    "id": "Z-W1o2SDDyAC",
    "ExecuteTime": {
     "end_time": "2024-11-07T07:18:15.058149Z",
     "start_time": "2024-11-07T07:18:15.046961Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提早結束\n"
   ],
   "metadata": {
    "id": "m3AHKBbGD8iI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ],
   "metadata": {
    "id": "iI_oewCpD__a",
    "ExecuteTime": {
     "end_time": "2024-11-07T07:18:20.317673Z",
     "start_time": "2024-11-07T07:18:20.295178Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 訓練\n"
   ],
   "metadata": {
    "id": "CKF4PMNbV3_L"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T07:18:22.614587Z",
     "start_time": "2024-11-07T07:18:22.376322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # 查看 PyTorch 所使用的 CUDA 版本\n",
    "print(torch.cuda.is_available())  # 檢查是否可使用 CUDA\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import scipy\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "NUM_EPOCHS = 40\n",
    "BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('%f, %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BH6V0l4p0vVL",
    "outputId": "3d937efd-aba6-4024-a849-9bf4b226d858",
    "ExecuteTime": {
     "end_time": "2024-11-07T07:38:51.602112Z",
     "start_time": "2024-11-07T07:25:51.199539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_15512\\3035275046.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if float(np.random.rand(1)) > 0.5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.183674, 0.027080\n",
      "0.037527, 0.011598\n",
      "0.025441, 0.027195\n",
      "0.022191, 0.016323\n",
      "0.010271, 0.010801\n",
      "0.010094, 0.009909\n",
      "0.007082, 0.006737\n",
      "0.010999, 0.010518\n",
      "0.009786, 0.006716\n",
      "0.007229, 0.007962\n",
      "0.007048, 0.006787\n",
      "0.008507, 0.006765\n",
      "0.006415, 0.007098\n",
      "0.005395, 0.007559\n",
      "0.006857, 0.006943\n",
      "0.004813, 0.007227\n",
      "0.006151, 0.007539\n",
      "0.004676, 0.006225\n",
      "0.005738, 0.005222\n",
      "0.004904, 0.007758\n",
      "0.004526, 0.004814\n",
      "0.005273, 0.007263\n",
      "0.004427, 0.005209\n",
      "0.004980, 0.006562\n",
      "0.005174, 0.008161\n",
      "0.005986, 0.006507\n",
      "0.005013, 0.008764\n",
      "0.003806, 0.006984\n",
      "0.003806, 0.006883\n",
      "0.003666, 0.006932\n",
      "0.004157, 0.009073\n",
      "0.006385, 0.006268\n",
      "0.004135, 0.005613\n",
      "0.003219, 0.005438\n",
      "0.002588, 0.004988\n",
      "0.003066, 0.005204\n",
      "0.015437, 0.104340\n",
      "0.023970, 0.011067\n",
      "0.008832, 0.007699\n",
      "0.006551, 0.010238\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T08:01:18.732531Z",
     "start_time": "2024-11-07T08:00:15.032267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# 設定資料夾路徑\n",
    "image_folder = \"1100\"\n",
    "image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 設定 widget 以顯示圖片\n",
    "widget_width = 224\n",
    "widget_height = 224\n",
    "image_widget = widgets.Image(format='jpeg', width=widget_width, height=widget_height)\n",
    "display(image_widget)\n",
    "\n",
    "# 設定裝置 (若有 GPU 可用)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 加載模型\n",
    "model = torchvision.models.resnet18()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(\"lane_detection_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()  # 將模型設定為推論模式\n",
    "\n",
    "# 圖片預處理和顯示函數\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4691, 0.4032, 0.4579], [0.1740, 0.1485, 0.1688])\n",
    "])\n",
    "\n",
    "def bgr8_to_jpeg(image):\n",
    "    _, jpeg = cv2.imencode('.jpg', image)\n",
    "    return jpeg.tobytes()\n",
    "\n",
    "def process_image(image_path):\n",
    "    # 讀取圖片並進行預處理\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        x, y = output[0].cpu().numpy()\n",
    "    print(x,y)\n",
    "    # 映射預測的 (x, y) 到圖片的像素坐標，假設 x 和 y 是 [0, 1] 範圍內的預測\n",
    "    x_pixel = int(x * widget_width / 2 + widget_width / 2)\n",
    "    y_pixel = int(y * widget_height / 2 + widget_height / 2)\n",
    "    \n",
    "    # 在圖片上繪製預測結果\n",
    "    display_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # 確保格式正確\n",
    "    cv2.circle(display_image, (x_pixel, y_pixel), 5, (0, 255, 0), -1)  # 綠色點表示預測位置\n",
    "    \n",
    "    # 更新 widget 顯示處理後的圖片\n",
    "    image_widget.value = bgr8_to_jpeg(display_image)\n",
    "\n",
    "# 對資料夾中的每張圖片進行處理\n",
    "for image_file in image_files:\n",
    "    process_image(image_file)\n",
    "    time.sleep(1)  # 暫停以觀察每張圖片的結果\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce80a1bd6df14231a549da1c5c4224ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_15512\\1303108273.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_detection_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8780172 0.717673\n",
      "-0.8778583 0.7950877\n",
      "-0.9178627 0.84937817\n",
      "-0.85928243 0.5425702\n",
      "-0.88263667 0.59387165\n",
      "-0.880228 0.51428795\n",
      "-0.8686982 0.5173032\n",
      "-0.8280813 0.6212539\n",
      "-0.8869151 0.7009606\n",
      "-0.87433624 0.73298657\n",
      "-0.90913576 0.73528033\n",
      "-0.8728177 0.7922363\n",
      "-0.8945904 0.79394495\n",
      "-0.8679162 0.59366095\n",
      "-0.8702812 0.6430227\n",
      "-0.89169306 0.68821\n",
      "-0.8203165 0.72577375\n",
      "-0.8397779 0.77373385\n",
      "-0.8263668 0.7792012\n",
      "-0.8380804 0.6266011\n",
      "-0.8518399 0.68623847\n",
      "-0.87821007 0.8074782\n",
      "-0.83594745 0.6760291\n",
      "-0.90653384 0.72998506\n",
      "-0.8525966 0.7429267\n",
      "-0.8472894 0.62026083\n",
      "-0.7981664 0.59726024\n",
      "-0.8309813 0.6323238\n",
      "-0.835523 0.65371215\n",
      "-0.832945 0.78938013\n",
      "-0.83614784 0.61466235\n",
      "-0.75203913 0.6402804\n",
      "-0.80812865 0.63145787\n",
      "-0.7673331 0.65914136\n",
      "-0.7881774 0.6809462\n",
      "-0.85062516 0.713171\n",
      "-0.79798067 0.67826164\n",
      "-0.8031116 0.70067656\n",
      "-0.84401244 0.74708265\n",
      "-0.849312 0.615506\n",
      "-0.8171975 0.6445993\n",
      "-0.82427174 0.6531872\n",
      "-0.842652 0.63734204\n",
      "-0.842652 0.63734204\n",
      "-0.92720217 0.72290987\n",
      "-0.8408798 0.6862918\n",
      "-0.7850078 0.6542239\n",
      "-0.7810371 0.7354498\n",
      "-0.8059509 0.81307214\n",
      "-0.8220077 0.88874984\n",
      "-0.78412 0.61212397\n",
      "-0.743725 0.66328245\n",
      "-0.79949516 0.6566302\n",
      "-0.849312 0.615506\n",
      "-0.76567286 0.6294445\n",
      "-0.7364035 0.63714\n",
      "-0.79072434 0.6434243\n",
      "-0.7846542 0.63284343\n",
      "-0.6878509 0.5952013\n",
      "-0.79200387 0.69512314\n",
      "-0.76835847 0.71072084\n",
      "-0.7837823 0.7257542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 65\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image_file \u001B[38;5;129;01min\u001B[39;00m image_files:\n\u001B[0;32m     64\u001B[0m     process_image(image_file)\n\u001B[1;32m---> 65\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 暫停以觀察每張圖片的結果\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T21:54:29.134157Z",
     "start_time": "2024-11-07T21:54:22.487093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 初始化 ResNet-18 模型并调整最后一层以匹配您的任务（假设输出为2个类别）\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # 根据您的任务调整输出类别数\n",
    "model = model.to(device)\n",
    "\n",
    "# 加载已训练的权重\n",
    "model_path = \"lane_detection_model.pth\"  # 替换为您的模型文件路径\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # 设置模型为推理模式\n",
    "\n",
    "# 定义剪枝函数\n",
    "def prune_model(model, amount=0.2):\n",
    "    \"\"\"\n",
    "    对模型的卷积层和全连接层应用剪枝。\n",
    "    :param model: 要剪枝的模型\n",
    "    :param amount: 剪枝比例，例如0.2表示剪掉20%的参数\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')  # 移除剪枝的重参数，保留剪枝后的权重\n",
    "\n",
    "# 应用剪枝\n",
    "prune_model(model, amount=0.2)\n",
    "\n",
    "# 测试剪枝后的模型\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(device)  # 根据您的输入尺寸调整\n",
    "    output = model(test_input)\n",
    "    print(\"Model output after pruning:\", output)\n",
    "\n",
    "# 保存剪枝后的模型\n",
    "torch.save(model.state_dict(), \"pruned_lane_detection_model.pth\")\n",
    "print(\"Pruned model saved as pruned_lane_detection_model.pth\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chino\\AppData\\Local\\Temp\\ipykernel_63304\\326031744.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output after pruning: tensor([[-0.2314,  0.6814]], device='cuda:0')\n",
      "Pruned model saved as pruned_lane_detection_model.pth\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ]
}
